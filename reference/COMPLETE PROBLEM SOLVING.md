# COMPLETE PROBLEM SOLVING PHASE ARCHITECTURE
(State-Flow Graph Version)

==================================================

## 0Ô∏è‚É£ TRIGGER
Dominant Function: Awareness

Purpose:
Recognize that a problem or opportunity exists.

- Detect gap, anomaly, inefficiency, confusion, or goal
- Identify why it matters
- Decide whether it deserves structured solving

Inputs:
- Reflection results
- Deployment outcomes
- Reinforcement updates
- New task initiation

Input From:
- DEPLOYMENT
- REFLECTION
- REINFORCEMENT
- (Start State)

Outputs:
‚Üí Problem signal

Output To:
- FRAMING
- (Terminate / Ignore)

==================================================

## 1Ô∏è‚É£ FRAMING(Problem Definition Layer)
Dominant Function: Clarification

Purpose:
Define exactly what is being solved.

- Precise restatement
- Define success criteria
- Identify inputs, outputs, objectives
- Define scope and boundaries
- Identify stakeholders
- Classify problem type

Inputs:
- Problem signal

Input From:
- TRIGGER
- VALIDATION (if structural failure detected)
- DECISION (if revision required)

Outputs:
‚Üí Well-defined problem statement
‚Üí Success criteria
‚Üí Scope boundaries

Output To:
- INFORMATION & MODELING
- SEARCH SPACE DEFINITION

==================================================

## 2Ô∏è‚É£ INFORMATION & MODELING
Dominant Function: Structural Understanding

Purpose:
Build structured representation of the problem

A. Information Gathering
    - Collect givens
    - Separate knowns / unknowns / unknowables
    - Ask clarifying questions
    - Acquire missing data

B. Representation Engineering
    - Choose model form 
        -algebraic
        -geometric
        -graph
        -probabilistic
        -algorithmic
        -simulation
C. Structural Mapping
    - Variables (what can change)
    - Constraints (limits)
    - Relationships (interactions)
    - Goals (what must be achieved)
    - Invariants / symmetries
    - Degrees of freedom
D. Causal Analysis
    - Identify root causes
    - Distinguish symptom vs cause
    - Map cause‚Äìeffect chains
    - Identify feedback loops
    - Analyze how root causes interact

Inputs:
- Problem definition
- Scope boundaries

Input From:
- FRAMING
- GENERATION (if new data required)
- VALIDATION (if inconsistency found)

Outputs:
‚Üí Structured model
‚Üí Variable map
‚Üí Known/unknown list
‚Üí Causal map

Output To:
- SEARCH SPACE DEFINITION
- GENERATION
- EVALUATION (if model already sufficient)

==================================================

## 3Ô∏è‚É£ SEARCH SPACE DEFINITION
Dominant Function: Boundary Control

Purpose:
Define solution space.

- Explicit constraints
    - Logical
    - Physical
    - Computational
    - Resource
    - Ethical
    - Adversarial
- Assumptions
    - Explicit vs implicit
    - Domain vs temporary
    - Reversible vs irreversible
- Define allowed vs forbidden regions
- Identify edge and extreme cases
- Detect underdetermined vs overdetermined regimes


Inputs:
- Structured model
- Success criteria

Input From:
- INFORMATION & MODELING
- FRAMING

Outputs:
‚Üí Bounded solution space
‚Üí Validity rules

Output To:
- GENERATION
- EVALUATION

==================================================

## 4Ô∏è‚É£ GENERATION
Dominant Function: Divergent Exploration

Purpose:
Create candidate solutions.

- Generate multiple approaches
- Use analogy and transfer
- Extremal and limiting reasoning
- Controlled randomness
- Combine and cluster ideas
- Form competing hypotheses / candidates

Inputs:
- Structured model
- Constraints
- Boundaries

Input From:
- INFORMATION & MODELING
- SEARCH SPACE DEFINITION
- CONSTRUCTION (if build failure)
- EVALUATION (if all candidates rejected)

Outputs:
‚Üí Candidate solution set

Output To:
- EVALUATION
- INFORMATION & MODELING (if gap detected)

==================================================

## 5Ô∏è‚É£ EVALUATION
Dominant Function: Convergent Selection

Purpose:
Select the strongest candidate.

- Compare candidates using criteria:
    - Constraint satisfaction
    - Explanatory power
    - Simplicity
    - Generality
    - Risk vs benefit
    - Test cost vs information gain
- Eliminate weak candidates
- Select primary candidate (optionally backup)


Inputs:
- Candidate set
- Constraints
- Success criteria

Input From:
- GENERATION
- SEARCH SPACE DEFINITION
- INFORMATION & MODELING

Outputs:
‚Üí Ranked options
‚Üí Selected candidate

Output To:
- CONSTRUCTION
- GENERATION (if none viable)
- INFORMATION & MODELING (if missing data)

==================================================

## 6Ô∏è‚É£ CONSTRUCTION
Dominant Function: Implementation

Purpose:
Build the selected solution.

- Formal derivation / proof
- Algorithm implementation
- Prototype building
- Simulation execution
- Step-by-step development


Inputs:
- Selected candidate

Input From:
- EVALUATION

Outputs:
‚Üí Concrete artifact
‚Üí Executable solution

Output To:
- VALIDATION
- GENERATION (if structural failure)

==================================================

## 7Ô∏è‚É£ VALIDATION
Dominant Function: Critical Testing

Purpose:
Ensure correctness and robustness.

- Internal logical consistency
- Edge and boundary case testing
- Stress and adversarial tests
- Sensitivity analysis
- Compare predicted vs actual behavior
- Redundancy checks


Inputs:
- Constructed artifact
- Success criteria

Input From:
- CONSTRUCTION
- EVALUATION (theoretical validation case)

Outputs:
‚Üí Verified solution
OR
‚Üí Failure report

Output To:
- DECISION
- GENERATION (if candidate invalid)
- INFORMATION & MODELING (if structural flaw)
- FRAMING (if problem misdefined)

==================================================

## 8Ô∏è‚É£ DECISION
Dominant Function: Commitment

Purpose:
Commit or revise.

- Accept / Reject / Revise
- Choose trade-offs explicitly
- Assess confidence level
- Document limitations
Inputs:
- Validation result

Input From:
- VALIDATION

Outputs:
‚Üí Committed solution
OR
‚Üí Revision directive

Output To:
- DEPLOYMENT (if accepted)
- GENERATION
- INFORMATION & MODELING
- FRAMING

==================================================

## 9Ô∏è‚É£ DEPLOYMENT
Dominant Function: Application

Purpose:
Apply in real context.

- Implement in environment
- Communicate assumptions and limits
- Monitor early performance


Inputs:
- Committed solution

Input From:
- DECISION

Outputs:
‚Üí Real-world performance results
‚Üí Operational feedback

Output To:
- REFLECTION
- TRIGGER (if new problem emerges)

==================================================

## üîü REFLECTION
Dominant Function: Meta-Cognition

Purpose:
Extract learning from outcome.

- What worked?
- What failed?
- Where were assumptions wrong?
- Where was time wasted?
- Which signals were ignored?


Inputs:
- Deployment outcomes
- Validation results

Input From:
- DEPLOYMENT
- VALIDATION

Outputs:
‚Üí Lessons learned
‚Üí Model updates

Output To:
- GENERALIZATION
- TRIGGER (if systemic issue found)

==================================================

## 1Ô∏è‚É£1Ô∏è‚É£ GENERALIZATION
Dominant Function: Abstraction

Purpose:
Extract reusable structure.

- Identify structural pattern
- Extract reusable template
- Identify triggering cues
- Compress into schema or checklist

Inputs:
- Lessons learned

Input From:
- REFLECTION

Outputs:
‚Üí Schema / reusable structure

Output To:
- REINFORCEMENT
- TRIGGER (future use)

==================================================

## 1Ô∏è‚É£2Ô∏è‚É£ REINFORCEMENT
Dominant Function: Capability Upgrade
step_10(introspection+retrospection)
[Reinforcement](https://github.com/negzero/skills/blob/main/cp_road_map2.md)
Purpose:
Improve internal toolkit.

- Improve tools
- Improve evaluation criteria
- Improve search heuristics
- Expand model library
- Reduce future cognitive load
based on feeback from step_10
Inputs:
- Schema
- Reflection results

Input From:
- GENERALIZATION
- REFLECTION

Outputs:
‚Üí Upgraded internal system

Output To:
- TRIGGER (next problem cycle)

==================================================================

MACRO FLOW:

Trigger
‚Üí Framing
‚Üí Information & Modeling
‚Üí Search Space Definition
‚Üí Generation
‚Üí Evaluation
‚Üí Construction
‚Üí Validation
‚Üí Decision
‚Üí Deployment
‚Üí Reflection
‚Üí Generalization
‚Üí Reinforcement
‚Üí (Next Problem)

==================================================================

For EACH Phase in PS_Phases:

# CANONICAL ACTION ARCHITECTURE (PS-Integrated Version)

User (Actor)
  ‚Üì
Identity / Role (who am I in this action?)
  ‚Üì
Context (environment / situation / domain)
  ‚Üì
Stakeholders / Audience (who is affected? who evaluates success?)
  ‚Üì
Purpose (why act? meaning / value)
  ‚Üì
Job / Goal (what outcome must be achieved?)
  ‚Üì
Success Definition (acceptance criteria / "done" condition)
  ‚Üì
Priority (importance relative to other goals)
  ‚Üì
Time Horizon (short-term / long-term orientation)
  ‚Üì
Constraints (rules, limits, time, policies)
  ‚Üì
Resources (time, attention, knowledge, energy, data available)
  ‚Üì
Dependencies (information required, approvals, prior results, prerequisite knowledge)
  ‚Üì
Preconditions (what must be true before starting)
  ‚Üì
Assumptions (what is believed to be true)
  ‚Üì
Evidence Base (what supports assumptions?)
  ‚Üì
Uncertainty Map (knowns / unknowns / unknown unknowns)
  ‚Üì
Risk Awareness (what can go wrong conceptually or procedurally?)
  ‚Üì
Paradigm (general problem-solving lens)
  ‚Üì
Strategy (high-level plan)
  ‚Üì
Alternatives Considered (other possible approaches)
  ‚Üì
Selection Rationale (why this strategy?)
  ‚Üì
Tactics (concrete moves inside strategy)
  ‚Üì
Toolset (mechanisms used: models, frameworks, heuristics, representations)
  ‚Üì
Techniques / Methods (rules for using the tools)
  ‚Üì
Skill Needed (capability required to execute techniques correctly)
  ‚Üì
Algorithm (abstract step logic, if applicable)
  ‚Üì
Procedure (concrete step-by-step execution)
  ‚Üì
Checkpoints / Guardrails (self-check rules)
  ‚Üì
Error Handling (what to do if reasoning fails or result contradicts goal)
  ‚Üì
Metric(s) (how success is measured)
  ‚Üì
Instrumentation (how measurement is obtained)
  ‚Üì
Validation / Verification (correctness + robustness checks)
  ‚Üì
Feedback (results from execution)
  ‚Üì
Reflection (why it worked or failed)
  ‚Üì
Decision Gate (Continue? Revise? Pivot?)
  ‚Üì
Communication / Output Structuring (how result is expressed)
  ‚Üì
Documentation (what to record for reuse)
  ‚Üì
Skill Update (capability adjustment)
  ‚Üì
Practice Plan (deliberate repetition)
  ‚Üì
Knowledge Extraction (structural lesson learned)
  ‚Üì
Abstraction / Schema Update (generalization into reusable template)
  ‚Üì
Transfer Plan (where else this applies)



===========================================================================

# PHASE CONTROL STRATEGIES (Meta-Layer)

Execution Modes:

1Ô∏è‚É£ Sequential Mode
Trigger ‚Üí Framing ‚Üí Modeling ‚Üí ...

2Ô∏è‚É£ Parallel Mode
Run:
- Information + Modeling simultaneously
- Generation + Evaluation (rapid prototyping loop)
- Validation during Construction

3Ô∏è‚É£ Recursive Mode
Inside any phase:
    If subproblem detected ‚Üí
        Call PS_Phases on subproblem
        Return result to parent phase

4Ô∏è‚É£ Skipping Mode
Skip a phase IF:
- Exit criteria already satisfied
- Problem type does not require it
- Information is trivial

5Ô∏è‚É£ Fast Loop Mode (Compressed Solving)
Framing ‚Üí Modeling ‚Üí Generate ‚Üí Quick Validate
(Used for low-risk problems)

6Ô∏è‚É£ Deep Mode (High Stakes)
Full execution with strict validation and reflection

7Ô∏è‚É£ Early Validation Mode
Validation happens inside:
- Modeling (check structural coherence)
- Generation (test small pieces early)
- Construction (test per module)

8Ô∏è‚É£ Dynamic Phase Re-entry
From any phase:
    If contradiction detected ‚Üí
        Return to the earliest phase where assumption broke
        
        
===========================================================================

Key Structural Upgrade

Instead of:

PS = Timeline

Make it:

PS = Phase Graph

Each phase is a node.

Edges define allowed transitions.

Example:

Evaluation ‚Üí Generation

Validation ‚Üí Modeling

Construction ‚Üí Generation

Deployment ‚Üí Framing (if stakeholder rejects)

This gives flexibility without chaos.
